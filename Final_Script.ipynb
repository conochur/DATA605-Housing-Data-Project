{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d616b802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:3078: FutureWarning: The frame.append method is deprecated and will be removed fromdask in a future version. Use dask.dataframe.concat instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#####IMPORT HOUSING DATA FROM S3#####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import dask as dd\n",
    "from dask.dataframe import DataFrame\n",
    "\n",
    "#declare the states being used for the analysis\n",
    "#States with Data = ['AZ', 'CA', 'CT', 'FL', 'GA', 'IL', 'IA', 'MD', 'MA', 'MI', 'MN', 'MO', 'NE', 'NV', 'NJ', 'NC', 'OH', 'OK', 'OR', 'PA', 'SC', 'TN', 'TX', 'VA']\n",
    "#States = ['AZ', 'CA', 'CT', 'FL', 'GA', 'IL', 'IA', 'MD', 'MA', 'MI', 'MN', 'MO', 'NE', 'NV', 'NJ', 'NC', 'OH', 'OK', 'OR', 'PA', 'SC', 'TN', 'TX', 'VA']\n",
    "States = ['DC']\n",
    "df_all = pd.DataFrame()\n",
    "df_all = dd.dataframe.from_pandas(df_all, npartitions=10)\n",
    "\n",
    "#pull chosen state's data from S3 and append to dask dataframe\n",
    "for State in States:\n",
    "    try:\n",
    "        bucket = 'housingdata123'\n",
    "        data_key = 'HousingData_Dolt_{}.csv'.format(State)\n",
    "        data_location = 's3://{}/{}'.format(bucket,data_key)\n",
    "\n",
    "        df = dd.dataframe.read_csv(data_location, storage_options={'key': 'AKIA5OMWNVBKXKF5SI7O',\n",
    "                                       'secret': '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'}, \n",
    "                                   dtype={'zip5': 'object', 'physical_address': 'object','property_id': \n",
    "                                          'object', 'sale_date':'object', 'year_built': 'object','book': 'object',\n",
    "                                          'buyer_name': 'object','page': 'object','property_type': 'object',\n",
    "                                          'sale_type': 'object','seller_name': 'object','city': 'object', \n",
    "                                          'num_units': 'object','county': 'object'})\n",
    "    \n",
    "        df_all = df_all.append(df)\n",
    "        \n",
    "    except:\n",
    "        print(\"Import failed for the following state: {}\".format(State))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3ba9c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####CLEANUP TIME#####\n",
    "\n",
    "#choose years for analysis; if you want all years, type 'all'\n",
    "Years = ['2016','2017','2018','2019']\n",
    "#Years= ['all']\n",
    "\n",
    "#drop extraneous columns\n",
    "df_all.drop(columns=['property_type','sale_price',\n",
    "                     'seller_name','buyer_name',\n",
    "                     'num_units','year_built',\n",
    "                     'source_url','book','page',\n",
    "                     'sale_type'])\n",
    "\n",
    "#drop rows with 0 for their sale price\n",
    "df_all = df_all[df_all.sale_price != 0]\n",
    "\n",
    "#reformat the sales dates to only include the year\n",
    "df_all['sale_date'] = df_all['sale_date'].str[0:4]\n",
    "df_all['sale_date'] = df_all['sale_date'].astype(str)\n",
    "\n",
    "if 'all' not in Years:\n",
    "    \n",
    "    #drop rows that aren't in the desired year range\n",
    "    df_all = df_all[df_all.sale_date.isin(Years)]\n",
    "\n",
    "Years = list(set(df_all['sale_date']))\n",
    "\n",
    "#group and average housing prices by zip code\n",
    "house_price_avg = df_all.groupby(['zip5','sale_date']).agg({'sale_price': ['mean','count']}).compute()\n",
    "house_price_avg = house_price_avg.droplevel(level = 0,axis=1)\n",
    "house_price_avg = house_price_avg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cd2ec63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a time series analysis on the housing data\n",
    "#parallelizing model prediction for several parameters: feature number, categorical variable buckets\n",
    "#comparing the time series forecasting to the census bureau modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0cd5fcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:3078: FutureWarning: The frame.append method is deprecated and will be removed fromdask in a future version. Use dask.dataframe.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: DP05, 2017, Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:3078: FutureWarning: The frame.append method is deprecated and will be removed fromdask in a future version. Use dask.dataframe.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: DP05, 2019, Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:3078: FutureWarning: The frame.append method is deprecated and will be removed fromdask in a future version. Use dask.dataframe.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: DP05, 2018, Completed\n",
      "Table: DP05, 2016, Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:3078: FutureWarning: The frame.append method is deprecated and will be removed fromdask in a future version. Use dask.dataframe.concat instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#####IMPORT CHOSEN CENSUS BUREAU DATA#####\n",
    "\n",
    "#####melt the dataframes, add a year column, append to consolidated dataframe, edit, and then unmelt\n",
    "\n",
    "import requests # request http, api\n",
    "import pandas as pd # tabluar data\n",
    "import dask as dd\n",
    "from dask.dataframe import DataFrame\n",
    "\n",
    "#create a state code dictionary for the census bureau API\n",
    "api_states= ['AL','AK','AZ','AR','CA','CO','CT','DE','DC','FL','GA','HI','ID','IL','IN','IA','KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY']\n",
    "api_codes= [1,2,4,5,6,8,9,10,11,12,13,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,44,45,46,47,48,49,50,51,53,54,55,56]\n",
    "zip_iterator = zip(api_states, api_codes)\n",
    "API_State_Codes = dict(zip_iterator)\n",
    "\n",
    "#create list of codes associated with original state query\n",
    "API_List = ''\n",
    "for state in States:\n",
    "    API_List = API_List + str(API_State_Codes[state]) + ','\n",
    "\n",
    "API_List = API_List[0:-1]\n",
    "\n",
    "#declare census api key\n",
    "census_api_key = \"35bea501f0c96a696cc609f9cb32b27e6541fbae\"\n",
    "\n",
    "#declare tables and years\n",
    "Tables = ['DP05']\n",
    "Consol_Zip_Table = pd.DataFrame()\n",
    "Consol_Zip_Table = dd.dataframe.from_pandas(Consol_Zip_Table, npartitions=10)\n",
    "        \n",
    "for Year in Years:\n",
    "    for ID in Tables:\n",
    "        \n",
    "        #subject tables\n",
    "        try:\n",
    "            if str(ID[0]) == 'S':\n",
    "\n",
    "                url = \"https://api.census.gov/data/{0}/acs/acs5/subject?get=NAME,group({1})&for=zip%20code%20tabulation%20area:*&in=state:{2}&key={3}\"\\\n",
    "                .format(Year, ID ,census_api_key)\n",
    "\n",
    "                response = requests.request(\"GET\", url)\n",
    "\n",
    "        except:\n",
    "                print('Subject Table: ' + str(ID) + ', ' + str(Year) + ', Failed')\n",
    "                continue\n",
    "\n",
    "        #data profile tables\n",
    "        try:\n",
    "            if str(ID[0]) == 'D':\n",
    "\n",
    "                url = \"https://api.census.gov/data/{0}/acs/acs5/profile?get=NAME,group({1})&for=zip%20code%20tabulation%20area:*&in=state:{2}&key={3}\"\\\n",
    "                .format(Year, ID , API_List, census_api_key)\n",
    "\n",
    "                response = requests.request(\"GET\", url)\n",
    "                \n",
    "        except:\n",
    "                print('Data Profile Table: ' + str(ID) + ', ' + str(Year) + ', Failed')\n",
    "                continue\n",
    "\n",
    "        #detailed tables\n",
    "        try:\n",
    "            if str(ID[0]) == 'B':\n",
    "\n",
    "                url = \"https://api.census.gov/data/{0}/acs/acs5?get=NAME,group({1})&for=zip%20code%20tabulation%20area:*&in=state:{2}&key={3}\"\\\n",
    "                .format(Year, ID ,census_api_key)\n",
    "\n",
    "                response = requests.request(\"GET\", url)\n",
    "                \n",
    "        except:\n",
    "                print('Detailed Table: ' + str(ID) + ', ' + str(Year) + ', Failed')\n",
    "                continue\n",
    "                \n",
    "                \n",
    "        #create pandas dataframe, remove duplicate columns\n",
    "        df = pd.DataFrame(response.json()[1:], columns=response.json()[0])\n",
    "        df = df.loc[:,~df.columns.duplicated()]\n",
    "                \n",
    "        #add year column, melt into a long form\n",
    "        Remove_Col = ['NAME','GEO_ID','state','zip code tabulation area'] \n",
    "        Col = list(df.columns)\n",
    "        Col = [ele for ele in Col if ele not in Remove_Col]\n",
    "        \n",
    "        df_melt = df.melt(id_vars=['zip code tabulation area'], value_vars= Col, ignore_index=False)\n",
    "        df_melt['YEAR'] = Year\n",
    "        \n",
    "        #remove margin of error variables\n",
    "        df_melt = df_melt[df_melt['variable'].str.contains(\"M\")==False]\n",
    "\n",
    "        #append to consolidated dataframe\n",
    "        Consol_Zip_Table = Consol_Zip_Table.append(df_melt)\n",
    "        print('Table: ' + str(ID) + ', ' + str(Year) + ', Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2a34540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop non numeric rows\n",
    "Consol_Zip_Table['value'] = Consol_Zip_Table['value'].apply(pd.to_numeric, errors='coerce', meta=('value', 'float64')).fillna(0).astype(float).dropna()\n",
    "        \n",
    "#remove all non percent rows\n",
    "Consol_Zip_Table = Consol_Zip_Table[Consol_Zip_Table['value'].astype(float) < 100]\n",
    "Consol_Zip_Table = Consol_Zip_Table[Consol_Zip_Table['value'].astype(float) > 0]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7fcda980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct the datatypes in preparation for the join\n",
    "Consol_Zip_Table = Consol_Zip_Table.compute()\n",
    "Consol_Zip_Table[\"YEAR\"] = Consol_Zip_Table[\"YEAR\"].astype(str)\n",
    "Consol_Zip_Table[\"zip code tabulation area\"] = Consol_Zip_Table[\"zip code tabulation area\"].astype(str)\n",
    "Consol_Zip_Table[\"variable\"] = Consol_Zip_Table[\"variable\"].astype(str)\n",
    "Consol_Zip_Table[\"value\"] = Consol_Zip_Table[\"value\"].astype(float)\n",
    "\n",
    "Final_Table = pd.pivot_table(pandas_df, index=['YEAR','zip code tabulation area'], columns=['variable'], values=['value'],aggfunc=np.sum,fill_value=0)\n",
    "Final_Table = Final_Table.droplevel(level = 0,axis=1)\n",
    "Final_Table = Final_Table.reset_index()\n",
    "\n",
    "#join the housing data and census data\n",
    "house_price_avg[\"sale_date\"] = house_price_avg[\"sale_date\"].astype(str)\n",
    "house_price_avg[\"zip5\"] = house_price_avg[\"zip5\"].astype(str)\n",
    "\n",
    "Merged_Matrix = Final_Table.join(house_price_avg, lsuffix=['zip code tabulation area','YEAR'], rsuffix=['zip5','sale_date'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa7054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
